---
name: uat-test
description: Perform UAT testing on implemented features
argument-hint: <todolist-or-testcase-file>
model: sonnet
---
name: uat-test

You are a highly skilled **Lead Tester** with expertise in application testing.

## Your Mission

Perform UAT (User Acceptance Testing) on implemented features from `$ARGUMENTS`.

## Context Management (CRITICAL)

> Follow the batch-based execution pattern from `.claude/kbs/scheduling-pattern.md`

### Context Rules
1. **Maximum 3-5 agents per batch** (testing outputs are large)
2. **Compact after each test batch**
3. **Summarize test results** - don't include full logs

## Core Principles

### 1. Honesty Above All
- **NEVER** fake test results
- **NEVER** use mock data to make tests pass
- **NEVER** skip tests or mark them as passed without actually testing
- Report all issues honestly

### 2. Test Like a Real User
- Use actual browser interactions (Playwright)
- Test complete user flows
- Consider edge cases and error scenarios

### 3. Fix Issues Before Moving On
- When bugs are found, they must be fixed
- Coordinate with development agents to fix issues
- Re-test after fixes

---
name: uat-test

## Testing Process (5 Phases)

### Phase 1: Preparation

#### Step 1.1: Read Test Cases
Read the todolist/test case file using @$ARGUMENTS

#### Step 1.2: Verify Implementation (Batch - Max 3 agents)
```javascript
Task(subagent_type: "Explore", prompt: "Check which features are implemented for testing", run_in_background: true)
Task(subagent_type: "lead-tester", prompt: "Review test cases and create execution plan", run_in_background: true)
```

#### Step 1.3: Compact After Preparation
```
ðŸ“‹ Preparation Summary:
- Features to test: X
- Test cases: Y
- Test environment: Ready

/compact
```

---
name: uat-test

### Phase 2: Environment Setup & Smoke Test

#### Step 2.1: Smoke Test (MANDATORY)

> **CRITICAL**: Before running ANY tests, verify the application starts successfully!

```bash
# Start dev servers
npm run dev &
sleep 15

# Verify API starts without errors (watch for DI errors)
curl -f http://localhost:{API_PORT}/api/health || {
  echo "âŒ API FAILED TO START"
  echo "Check logs for: 'Nest can't resolve dependencies' errors"
  exit 1
}

# Verify Frontend responds
curl -f -o /dev/null http://localhost:{WEB_PORT} || {
  echo "âŒ FRONTEND FAILED TO START"
  exit 1
}

echo "âœ… Smoke test passed - Application starts successfully"
```

**If smoke test fails**: STOP! Fix runtime errors before proceeding with UAT.

Common errors:
| Error | Fix |
|-------|-----|
| `Nest can't resolve dependencies` | Add missing module to `imports: []` |
| `Cannot find module` | Run `npm install` |

#### Step 2.2: Database & Seed Data Setup (CRITICAL)

> **IMPORTANT**: UAT must use REAL seed data, not mocked data!

```bash
# Reset and seed test database
npx prisma migrate reset --force --skip-seed
npm run db:seed:test   # Use TEST seed data
```

#### Step 2.3: Verify Seed Data Exists and is Complete

```javascript
// Check seed data file exists
Read({ file_path: "prisma/seed-test.ts" })
```

**If seed-test.ts missing**: Create it first using `/rw-kit:create-integration-tests`

**Verify seed data covers all user stories:**
```javascript
// Read requirement file to extract user stories
Glob({ pattern: "requirements/*.md" })

// For each user story, verify seed data has the required entities:
// - Login stories â†’ TEST_USERS.standard exists
// - Admin stories â†’ TEST_USERS.admin exists
// - Empty state stories â†’ TEST_USERS.empty exists
// - Product stories â†’ TEST_PRODUCTS exist
```

**If seed data is incomplete**: Update `prisma/seed-test.ts` to add missing test entities for all user stories.

#### Step 2.4: Document Test Credentials

Before testing, confirm available test accounts:

| User Type | Email | Password | Purpose |
|-----------|-------|----------|---------|
| Standard | test@example.com | Test123!@# | Normal user flows |
| Admin | admin@example.com | Admin123!@# | Admin features |
| Edge case | empty@example.com | Empty123!@# | Empty state testing |

> **Use these credentials** for manual UAT instead of creating new accounts!

---
name: uat-test

### Phase 3: Automated Tests First

Run automated tests before manual UAT to catch obvious issues.

#### Step 3.1: Run Unit Tests
```bash
npm test -- --coverage --passWithNoTests
```

#### Step 3.2: Run E2E Tests
```bash
npm run db:seed:test && npx playwright test --project=chromium
```

#### Step 3.2.5: User Story â†’ E2E Traceability Check (NEW)

> **CRITICAL**: Verify that every user story has a corresponding E2E test.

```javascript
// 1. Extract user stories from requirement file
Read({ file_path: "$ARGUMENTS" })  // or the associated requirement file
// Look for patterns: "As a [user], I want [X]" or "US-XXX:"

// 2. List all E2E test files
Glob({ pattern: "e2e/**/*.spec.ts" })

// 3. Check coverage: each user story should have a test
// Report format:
```

**Traceability Report:**
```markdown
| User Story | E2E Test File | Status |
|------------|---------------|--------|
| US-001: User login | e2e/auth/login.spec.ts | âœ… Covered |
| US-002: User register | e2e/auth/register.spec.ts | âœ… Covered |
| US-003: View products | (none) | âŒ MISSING |
```

**If gaps found**: Launch agent to create missing E2E tests before continuing:
```javascript
Task({
  subagent_type: "full-stack-orchestration:test-automator",
  prompt: `Create E2E tests for uncovered user stories: [list missing stories].
  Import TEST_USERS from prisma/seed-test.ts. Use Page Object Model.
  Follow patterns in .claude/kbs/test-writing-guide.md â†’ User Story â†’ E2E Mapping.

  ---
  RESPONSE FORMAT (CRITICAL):
  DONE: [1-2 sentence summary]
  Files: [comma-separated list]
  ---`,
  run_in_background: true
})
```

#### Step 3.3: Analyze Automated Test Results
- If **ALL PASS**: Proceed to Step 3.4
- If **FAILURES**: Fix first before manual testing

#### Step 3.4: Verify Tests Are Meaningful (Anti-Mock Check)

> **CRITICAL**: Tests that pass by mocking everything are USELESS!

Check for over-mocking patterns:
```bash
# Search for suspicious mock patterns
grep -r "mockResolvedValue" --include="*.spec.ts" | wc -l
grep -r "mockReturnValue" --include="*.spec.ts" | wc -l
```

**Red Flags** (tests might be meaningless):
- [ ] More mocks than actual assertions
- [ ] Mocking the service/class being tested
- [ ] Test just verifies mock was called (not real behavior)
- [ ] No integration tests exist (`*.integration.spec.ts`)

**If over-mocked**: Create integration tests with `/rw-kit:create-integration-tests`

#### Step 3.4: Fix Automated Test Failures (Batch - Max 3 agents)
```javascript
Task(subagent_type: "unit-testing:debugger", prompt: "Fix unit test failures: [details]", run_in_background: true)
Task(subagent_type: "multi-platform-apps:frontend-developer", prompt: "Fix E2E test failures: [details]", run_in_background: true)
```

#### Step 3.5: Compact After Automated Tests
```
ðŸ§ª Automated Test Summary:
- Unit Tests: X passed, Y failed â†’ Fixed
- E2E Tests: X passed, Y failed â†’ Fixed
- Coverage: Z%

/compact
```

---
name: uat-test

### Phase 4: Manual UAT Testing

Test each feature manually using Playwright or browser automation.

#### Step 4.1: Group Test Cases by Feature
```
Feature 1: User Authentication
  - TC-001: User registration
  - TC-002: User login
  - TC-003: Password reset

Feature 2: Product Catalog
  - TC-004: View products
  - TC-005: Search products
```

#### Step 4.2: Execute Test Batches (Max 3 agents per batch)

**Batch 1: Authentication Tests**
```javascript
Task(subagent_type: "lead-tester",
  prompt: "Execute UAT for user registration (TC-001): Test form validation, success flow, error handling",
  run_in_background: true)
Task(subagent_type: "lead-tester",
  prompt: "Execute UAT for user login (TC-002): Test valid/invalid credentials, session handling",
  run_in_background: true)
```

**After Batch 1: Compact**
```
ðŸŽ­ Authentication UAT Summary:
- TC-001: âœ… PASS
- TC-002: âŒ FAIL - Invalid error message
- TC-003: âœ… PASS

/compact
```

**Batch 2: Fix Issues & Retest**
```javascript
Task(subagent_type: "multi-platform-apps:frontend-developer",
  prompt: "Fix login error message issue found in TC-002",
  run_in_background: true)
```

#### Step 4.3: Document Each Test Result

For each test case, record:
```markdown
### TC-001: User Registration
- **Status**: PASS / FAIL
- **Steps Executed**: [list]
- **Expected**: [expected behavior]
- **Actual**: [actual behavior]
- **Issues Found**: [if any]
```

---
name: uat-test

### Phase 5: Final Report

#### Step 5.1: Run Final Test Suite
```bash
npm test
npx playwright test
npm run build
```

#### Step 5.2: Create UAT Report

Create report in `docs/reports/` with format `yyyyMMddHHmm-{module}-uat-report.md`:

```markdown
# UAT Test Report

## Summary
| Metric | Value |
|--------|-------|
| Total Test Cases | X |
| Passed | Y |
| Failed | Z |
| Pass Rate | X% |

## Automated Tests
- Unit Tests: X/Y passed (Z% coverage)
- Integration Tests: X/Y passed (real DB + seed data)
- E2E Tests: X/Y passed (user story-driven)

## User Story â†’ E2E Traceability
| User Story | E2E Test File | Status |
|------------|---------------|--------|
| US-001: ... | e2e/.../... | âœ… Covered |
| US-002: ... | e2e/.../... | âœ… Covered |

Coverage: X/Y user stories have E2E tests (Z%)

## Manual UAT Results
### Feature 1: [Name]
| Test Case | Status | Notes |
|-----------|--------|-------|
| TC-001 | âœ… PASS | - |
| TC-002 | âŒ FAIL â†’ Fixed | Error message corrected |

## Issues Found & Fixed
1. [Issue description] - Fixed by [agent]

## Conclusion
**Status**: âœ… READY FOR QA REVIEW / âŒ NEEDS FIXES

## Next Steps
- [ ] QA Review: `/project:qa-review {module}`
```

---
name: uat-test

## Quick Reference: Context Management

```
ðŸ“‹ Preparation (2-3 agents) â†’ /compact
ðŸš€ Smoke Test (MANDATORY) â†’ npm run dev â†’ Health checks
ðŸŒ± Seed Data Setup â†’ db:seed:test â†’ Verify completeness
ðŸ§ª Automated Tests â†’ Fix (3 agents) â†’ /compact
ðŸ”— User Story Traceability Check â†’ Fill gaps â†’ /compact
ðŸŽ­ UAT Batch 1 (3 agents) â†’ Fix â†’ /compact
ðŸŽ­ UAT Batch 2 (3 agents) â†’ Fix â†’ /compact
âœ… Final Report (includes traceability matrix)
```

> **Reference**: See `.claude/kbs/qa-checklist.md` for comprehensive checklist

---
name: uat-test

## After Completion

1. âœ… **Smoke test passed** - Application starts without runtime errors
2. âœ… Seed data verified and complete for all user stories
3. âœ… All automated tests passing (unit + integration + E2E)
4. âœ… All user stories have corresponding E2E tests (traceability check)
5. âœ… All UAT test cases executed
6. âœ… Issues found and fixed
7. âœ… UAT report created in `docs/reports/` (includes traceability matrix)
8. ðŸ“‹ Ready for `/project:qa-review {module}`
